# ===============================
# PLANTDOC FASTER-RCNN FROM SCRATCH
# ===============================

import os
import xml.etree.ElementTree as ET
import torch
import torch.nn as nn
import torchvision
import numpy as np
import matplotlib.pyplot as plt

from PIL import Image
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.ops import MultiScaleRoIAlign
import torchvision.transforms as T

from sklearn.metrics import (
    accuracy_score, precision_score,
    recall_score, f1_score, confusion_matrix
)

# ===============================
# CONFIG
# ===============================
DATA_ROOT = "/kaggle/input/plantdoc-dataset"
# Based on the dataset structure, we need to handle different subdirectories
# Each class has its own folder with images

# Get all class folders (excluding 'test' directory)
class_folders = [f for f in os.listdir(DATA_ROOT) 
                if os.path.isdir(os.path.join(DATA_ROOT, f)) and f != 'test']
CLASSES = sorted(class_folders)

# For detection, we need to map class names to IDs
CLASSES = ["background"] + CLASSES
CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}
NUM_CLASSES = len(CLASSES)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EPOCHS = 50
BATCH_SIZE = 32
LR = 0.0001

# ===============================
# DATASET
# ===============================
class PlantDocDataset(Dataset):
    def __init__(self, data_root, transforms=None):
        self.data_root = data_root
        self.transforms = transforms
        
        # Collect all image paths and their corresponding class labels
        self.image_paths = []
        self.class_labels = []
        self.annotation_paths = []
        
        # Process each class folder
        for class_name in CLASSES[1:]:  # Skip 'background'
            class_path = os.path.join(data_root, class_name)
            if os.path.exists(class_path):
                # Get all images in this class folder
                for img_file in os.listdir(class_path):
                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(class_path, img_file)
                        
                        # Check for corresponding XML annotation
                        xml_file = img_file.rsplit('.', 1)[0] + '.xml'
                        xml_path = os.path.join(class_path, xml_file)
                        
                        # If XML exists, add to dataset
                        if os.path.exists(xml_path):
                            self.image_paths.append(img_path)
                            self.class_labels.append(class_name)
                            self.annotation_paths.append(xml_path)
        
        print(f"Found {len(self.image_paths)} images with annotations")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        xml_path = self.annotation_paths[idx]
        
        img = Image.open(img_path).convert("RGB")
        
        # Parse XML annotation
        tree = ET.parse(xml_path)
        root = tree.getroot()
        
        boxes, labels = [], []
        
        # Get image dimensions for normalization if needed
        width = int(root.find('size/width').text)
        height = int(root.find('size/height').text)
        
        for obj in root.findall('object'):
            # Get class label
            class_name = obj.find('name').text
            # Use the class name directly from XML if it exists in our classes
            # Otherwise, use the folder name
            if class_name in CLASS_TO_IDX:
                label = CLASS_TO_IDX[class_name]
            else:
                label = CLASS_TO_IDX[self.class_labels[idx]]
            
            # Get bounding box coordinates
            bndbox = obj.find('bndbox')
            xmin = float(bndbox.find('xmin').text)
            ymin = float(bndbox.find('ymin').text)
            xmax = float(bndbox.find('xmax').text)
            ymax = float(bndbox.find('ymax').text)
            
            # Normalize coordinates if needed
            # xmin, xmax = xmin/width, xmax/width
            # ymin, ymax = ymin/height, ymax/height
            
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(label)
        
        if len(boxes) == 0:
            # If no objects found, create a dummy box (this shouldn't happen with proper annotations)
            boxes = [[0, 0, 10, 10]]
            labels = [CLASS_TO_IDX[self.class_labels[idx]]]
        
        boxes = torch.tensor(boxes, dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.int64)
        
        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": torch.tensor([idx]),
            "area": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            "iscrowd": torch.zeros((len(boxes),), dtype=torch.int64)
        }
        
        if self.transforms:
            img = self.transforms(img)
        
        return img, target

# ===============================
# TRANSFORMS
# ===============================
def get_transforms(train=True):
    t = [T.Resize((512, 512)), T.ToTensor()]
    if train:
        t.extend([
            T.RandomHorizontalFlip(0.5),
            T.RandomRotation(10),
            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)
        ])
    return T.Compose(t)

# ===============================
# SPLITS
# ===============================
dataset = PlantDocDataset(DATA_ROOT, get_transforms(True))
n = len(dataset)
train_len = int(0.7 * n)
val_len = int(0.15 * n)
test_len = n - train_len - val_len

train_ds, val_ds, test_ds = random_split(dataset, [train_len, val_len, test_len])

# Apply validation/test transforms
val_ds.dataset.transforms = get_transforms(False)
test_ds.dataset.transforms = get_transforms(False)

def collate_fn(batch):
    return tuple(zip(*batch))

train_dl = DataLoader(train_ds, BATCH_SIZE, True, collate_fn=collate_fn)
val_dl   = DataLoader(val_ds, BATCH_SIZE, False, collate_fn=collate_fn)
test_dl  = DataLoader(test_ds, BATCH_SIZE, False, collate_fn=collate_fn)

# ===============================
# BACKBONE (FROM SCRATCH)
# ===============================
class CustomBackbone(nn.Module):
    def __init__(self):
        super().__init__()
        self.body = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(32), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64), nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(128), nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(256),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()
        )
        self.out_channels = 256
    
    def forward(self, x):
        return self.body(x)

# ===============================
# MODEL
# ===============================
backbone = CustomBackbone()

anchor_generator = AnchorGenerator(
    sizes=((32, 64, 128, 256),),
    aspect_ratios=((0.5, 1.0, 2.0),)
)

roi_pooler = MultiScaleRoIAlign(['0'], 7, 2)

model = FasterRCNN(
    backbone,
    num_classes=NUM_CLASSES,
    rpn_anchor_generator=anchor_generator,
    box_roi_pool=roi_pooler,
    min_size=512,
    max_size=512
).to(DEVICE)

optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)

# ===============================
# TRAINING
# ===============================
train_losses, train_acc, val_acc = [], [], []

def train_epoch(loader):
    model.train()
    loss_sum = 0
    for imgs, targets in loader:
        imgs = [i.to(DEVICE) for i in imgs]
        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]
        
        loss_dict = model(imgs, targets)
        losses = sum(loss for loss in loss_dict.values())
        
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        
        loss_sum += losses.item()
    return loss_sum / len(loader)

def evaluate(loader):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for imgs, targets in loader:
            imgs = [i.to(DEVICE) for i in imgs]
            outputs = model(imgs)
            
            for o, t in zip(outputs, targets):
                if len(o["labels"]) > 0:
                    # Get the prediction with highest score
                    if 'scores' in o and len(o['scores']) > 0:
                        best_idx = torch.argmax(o['scores']).item()
                        y_pred.append(o["labels"][best_idx].cpu().item())
                    else:
                        y_pred.append(o["labels"][0].cpu().item())
                    y_true.append(t["labels"][0].item())
    return y_true, y_pred

# Training loop
for epoch in range(EPOCHS):
    train_loss = train_epoch(train_dl)
    train_losses.append(train_loss)
    
    yt, yp = evaluate(train_dl)
    yv, ypv = evaluate(val_dl)
    
    if len(yt) > 0 and len(yp) > 0:
        train_acc.append(accuracy_score(yt, yp))
    else:
        train_acc.append(0.0)
    
    if len(yv) > 0 and len(ypv) > 0:
        val_acc.append(accuracy_score(yv, ypv))
    else:
        val_acc.append(0.0)
    
    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {train_loss:.4f} | "
          f"Train Acc: {train_acc[-1]:.3f} | Val Acc: {val_acc[-1]:.3f}")

# ===============================
# TEST METRICS
# ===============================
yt, yp = evaluate(test_dl)

if len(yt) > 0 and len(yp) > 0:
    print(f"\nTest Results on {len(yt)} samples:")
    print("Accuracy:", accuracy_score(yt, yp))
    print("Precision:", precision_score(yt, yp, average='macro', zero_division=0))
    print("Recall:", recall_score(yt, yp, average='macro', zero_division=0))
    print("F1 Score:", f1_score(yt, yp, average='macro', zero_division=0))
    print("Confusion Matrix:\n", confusion_matrix(yt, yp))
else:
    print("No predictions to evaluate on test set")

# ===============================
# PLOTS
# ===============================
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# Plot loss
ax1.plot(train_losses, label="Training Loss", color='red')
ax1.set_xlabel("Epoch")
ax1.set_ylabel("Loss")
ax1.set_title("Training Loss")
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot accuracy
ax2.plot(train_acc, label="Train Accuracy")
ax2.plot(val_acc, label="Val Accuracy")
ax2.set_xlabel("Epoch")
ax2.set_ylabel("Accuracy")
ax2.set_title("Training vs Validation Accuracy")
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ===============================
# SAVE MODEL
# ===============================
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'classes': CLASSES,
    'class_to_idx': CLASS_TO_IDX
}, "/kaggle/working/plantdoc_faster_rcnn.pth")
print("Model saved!")
