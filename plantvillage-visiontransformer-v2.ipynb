{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2542588,"sourceType":"datasetVersion","datasetId":1541807}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom tqdm import tqdm\nimport random\nfrom pathlib import Path\nimport math\nimport time\nimport seaborn as sns\nfrom sklearn.metrics import (\n    confusion_matrix, \n    classification_report, \n    accuracy_score,\n    precision_recall_fscore_support\n)\nimport pandas as pd\n\n# Configurations\nDATA_ROOT = \"/kaggle/input/plantvillage\"\nIMG_SIZE = 256\nPATCH_SIZE = 16\nNUM_CLASSES = 38\nBATCH_SIZE = 64\nEPOCHS = 25\nLEARNING_RATE = 0.0003\nWARMUP_EPOCHS = 5\nWEIGHT_DECAY = 0.05\nMIN_LR = 1e-6\nEARLY_STOP_PATIENCE = 7\nMIN_EPOCHS = 12\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Configuration:\")\nprint(f\"  Image Size: {IMG_SIZE}x{IMG_SIZE}\")\nprint(f\"  Batch Size: {BATCH_SIZE}\")\nprint(f\"  Epochs: {EPOCHS} (max, with early stopping)\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  Early Stop Patience: {EARLY_STOP_PATIENCE}\")\n\n# Dataset\nclass PlantVillageDataset(Dataset):\n    def __init__(self, data_root, transform=None, img_size=IMG_SIZE):\n        self.data_root = Path(data_root)\n        self.transform = transform\n        self.img_size = img_size\n        \n        possible_paths = [\n            self.data_root / \"PlantVillage\",\n            self.data_root / \"plantvillage\",\n            self.data_root / \"color\",\n            self.data_root / \"PlantVillageDataset\"\n        ]\n        \n        actual_path = None\n        for path in possible_paths:\n            if path.exists():\n                actual_path = path\n                break\n        \n        if actual_path is None:\n            for item in self.data_root.iterdir():\n                if item.is_dir() and \"train\" in str(item).lower():\n                    actual_path = item\n                    break\n        \n        if actual_path is None:\n            raise FileNotFoundError(f\"Could not find PlantVillage data in {data_root}\")\n        \n        train_path = actual_path / \"train\"\n        if train_path.exists():\n            actual_path = train_path\n        \n        self.base_path = actual_path\n        self.image_paths = []\n        self.labels = []\n        self.class_names = []\n        self.class_to_idx = {}\n        \n        class_dirs = sorted([d for d in self.base_path.iterdir() if d.is_dir()])\n        \n        if not class_dirs:\n            for item in self.base_path.iterdir():\n                if item.is_dir():\n                    class_dirs = sorted([d for d in item.iterdir() if d.is_dir()])\n                    if class_dirs:\n                        self.base_path = item\n                        break\n        \n        for idx, class_dir in enumerate(class_dirs):\n            class_name = class_dir.name\n            self.class_to_idx[class_name] = idx\n            self.class_names.append(class_name)\n            \n            image_files = []\n            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n                image_files.extend(list(class_dir.glob(ext)))\n            \n            for img_path in image_files:\n                self.image_paths.append(img_path)\n                self.labels.append(idx)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        \n        img = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            img = self.transform(img)\n        else:\n            img = T.Compose([\n                T.Resize((self.img_size, self.img_size)),\n                T.ToTensor(),\n                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            ])(img)\n        \n        return img, label\n\n\n# Data augmentation\ntrain_transform = T.Compose([\n    T.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n    T.RandomCrop(IMG_SIZE),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomVerticalFlip(p=0.15),\n    T.RandomRotation(15),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    T.RandomErasing(p=0.2, scale=(0.02, 0.1))\n])\n\nval_transform = T.Compose([\n    T.Resize((IMG_SIZE, IMG_SIZE)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Dataset loading\n\ndataset = PlantVillageDataset(DATA_ROOT, transform=train_transform)\nprint(f\"Total images: {len(dataset):,}\")\nprint(f\"Number of classes: {len(dataset.class_names)}\")\n\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(\n    dataset, [train_size, val_size, test_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nval_dataset.dataset.transform = val_transform\ntest_dataset.dataset.transform = val_transform\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                          num_workers=4, pin_memory=True, persistent_workers=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                        num_workers=4, pin_memory=True, persistent_workers=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                         num_workers=4, pin_memory=True, persistent_workers=True)\n\nprint(f\"Train: {len(train_dataset):,} | Val: {len(val_dataset):,} | Test: {len(test_dataset):,}\")\n\n\n# MODEL ARCHITECTURE\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=256, patch_size=16, in_channels=3, embed_dim=512):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n    \n    def forward(self, x):\n        B = x.shape[0]\n        x = self.proj(x)\n        x = x.flatten(2).transpose(1, 2)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.attn_drop = nn.Dropout(dropout)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.proj_drop = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.1):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        \n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim=512, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = MLP(embed_dim, mlp_hidden_dim, dropout=dropout)\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass ImprovedVisionTransformer(nn.Module):\n    def __init__(self, img_size=256, patch_size=16, in_channels=3, num_classes=38,\n                 embed_dim=512, depth=8, num_heads=8, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.pos_drop = nn.Dropout(dropout)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n        self.head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(embed_dim // 2, num_classes)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.patch_embed.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.patch_embed.pos_embed, std=0.02)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        x = self.norm(x)\n        x = x[:, 0]\n        x = self.head(x)\n        return x\n\n\n# Model initialization\nmodel = ImprovedVisionTransformer(\n    img_size=IMG_SIZE,\n    patch_size=PATCH_SIZE,\n    num_classes=NUM_CLASSES,\n    embed_dim=512,\n    depth=8,\n    num_heads=8,\n    mlp_ratio=4.0,\n    dropout=0.15\n).to(DEVICE)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params:,}\")\n\n\n# Training setup\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\ndef get_lr_scheduler(optimizer, warmup_epochs, total_epochs, min_lr=1e-6):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return (epoch + 1) / warmup_epochs\n        else:\n            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n            cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n            return max(cosine_decay, min_lr / LEARNING_RATE)\n    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nscheduler = get_lr_scheduler(optimizer, WARMUP_EPOCHS, EPOCHS, MIN_LR)\n\n\n# Training loop\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\ntrain_losses, train_accs = [], []\nval_losses, val_accs = [], []\nbest_val_acc = 0.0\nbest_epoch = 0\npatience_counter = 0\n\ntraining_start_time = time.time()\n\nfor epoch in range(EPOCHS):\n    epoch_start_time = time.time()\n    \n    # Training\n    model.train()\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1:02d}/{EPOCHS}\", leave=False)\n    for batch_idx, (images, labels) in enumerate(pbar):\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        if torch.isnan(loss):\n            print(f\"\\nWarning: NaN loss at epoch {epoch+1}, batch {batch_idx}\")\n            continue\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        if batch_idx % 20 == 0:\n            pbar.set_postfix({\n                'Loss': f\"{epoch_loss/(batch_idx+1):.4f}\",\n                'Acc': f\"{100.*correct/total:.2f}%\"\n            })\n    \n    train_loss = epoch_loss / len(train_loader)\n    train_acc = 100. * correct / total\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    val_loss = val_loss / len(val_loader)\n    val_acc = 100. * correct / total\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    \n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    epoch_time = time.time() - epoch_start_time\n    \n    print(f\"Epoch {epoch+1:02d}/{EPOCHS}: \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n          f\"LR: {current_lr:.6f} | Time: {epoch_time/60:.1f}m\")\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_epoch = epoch + 1\n        patience_counter = 0\n        \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': val_acc,\n            'train_acc': train_acc,\n            'class_names': dataset.class_names,\n            'train_losses': train_losses,\n            'train_accs': train_accs,\n            'val_losses': val_losses,\n            'val_accs': val_accs\n        }, \"/kaggle/working/best_model.pth\")\n        \n        print(f\"  âœ“ New best! Val Acc: {val_acc:.2f}%\")\n    else:\n        patience_counter += 1\n        print(f\"  No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})\")\n    \n    # Early stopping\n    if patience_counter >= EARLY_STOP_PATIENCE and epoch >= MIN_EPOCHS:\n        print(f\"\\nâ¹ Early stopping at epoch {epoch+1}\")\n        print(f\"  Best val acc: {best_val_acc:.2f}% at epoch {best_epoch}\")\n        break\n\ntotal_training_time = time.time() - training_start_time\n\nprint(f\"Training completed in {total_training_time/60:.1f} minutes\")\nprint(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n\n\n\n# EVALUATION - COLLECT PREDICTIONS\n\ncheckpoint = torch.load(\"/kaggle/working/best_model.pth\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nclass_names = dataset.class_names\n\ndef get_predictions(model, data_loader, device):\n    \"\"\"Get all predictions and true labels\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n            images = images.to(device)\n            outputs = model(images)\n            probs = torch.softmax(outputs, dim=1)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    return np.array(all_labels), np.array(all_preds), np.array(all_probs)\n\nprint(\"\\nCollecting predictions...\")\ntest_labels, test_preds, test_probs = get_predictions(model, test_loader, DEVICE)\nval_labels, val_preds, val_probs = get_predictions(model, val_loader, DEVICE)\n\n\n# CALCULATE METRICS\nprint(\"\\nCalculating metrics...\")\n\ndef calculate_metrics(y_true, y_pred):\n    accuracy = accuracy_score(y_true, y_pred) * 100\n    precision, recall, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, average='weighted', zero_division=0\n    )\n    precision_per_class, recall_per_class, f1_per_class, support_per_class = \\\n        precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision * 100,\n        'recall': recall * 100,\n        'f1': f1 * 100,\n        'precision_per_class': precision_per_class,\n        'recall_per_class': recall_per_class,\n        'f1_per_class': f1_per_class,\n        'support_per_class': support_per_class\n    }\n\ntest_metrics = calculate_metrics(test_labels, test_preds)\nval_metrics = calculate_metrics(val_labels, val_preds)\n\nprint(\"\\nðŸ“Š TEST SET METRICS:\")\nprint(f\"  Accuracy:  {test_metrics['accuracy']:.2f}%\")\nprint(f\"  Precision: {test_metrics['precision']:.2f}%\")\nprint(f\"  Recall:    {test_metrics['recall']:.2f}%\")\nprint(f\"  F1-Score:  {test_metrics['f1']:.2f}%\")\n\nprint(\"\\nðŸ“Š VALIDATION SET METRICS:\")\nprint(f\"  Accuracy:  {val_metrics['accuracy']:.2f}%\")\nprint(f\"  Precision: {val_metrics['precision']:.2f}%\")\nprint(f\"  Recall:    {val_metrics['recall']:.2f}%\")\nprint(f\"  F1-Score:  {val_metrics['f1']:.2f}%\")\n\n\n# CONFUSION MATRIX\nprint(\"\\nGenerating confusion matrices...\")\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, title, save_path):\n    cm = confusion_matrix(y_true, y_pred)\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    \n    plt.figure(figsize=(20, 18))\n    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Percentage (%)'}, linewidths=0.5)\n    \n    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n    plt.ylabel('True Label', fontsize=14)\n    plt.xlabel('Predicted Label', fontsize=14)\n    plt.xticks(rotation=45, ha='right', fontsize=8)\n    plt.yticks(rotation=0, fontsize=8)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    return cm\n\ntest_cm = plot_confusion_matrix(test_labels, test_preds, class_names,\n    \"Test Set Confusion Matrix\", \"/kaggle/working/confusion_matrix_test.png\")\n\nval_cm = plot_confusion_matrix(val_labels, val_preds, class_names,\n    \"Validation Set Confusion Matrix\", \"/kaggle/working/confusion_matrix_val.png\")\n\nprint(\"  âœ“ Confusion matrices saved\")\n\n\n# Training curves\nprint(\"\\nGenerating training curves...\")\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Loss\naxes[0, 0].plot(train_losses, label='Training Loss', linewidth=2.5, color='#e74c3c', alpha=0.8)\naxes[0, 0].plot(val_losses, label='Validation Loss', linewidth=2.5, color='#3498db', alpha=0.8)\naxes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\naxes[0, 0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold', pad=15)\naxes[0, 0].legend(fontsize=11)\naxes[0, 0].grid(True, alpha=0.3)\n\n# Accuracy\naxes[0, 1].plot(train_accs, label='Training Accuracy', linewidth=2.5, color='#e74c3c', alpha=0.8)\naxes[0, 1].plot(val_accs, label='Validation Accuracy', linewidth=2.5, color='#3498db', alpha=0.8)\naxes[0, 1].axhline(y=75, color='green', linestyle='--', linewidth=2, alpha=0.6, label='75% Target')\naxes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\naxes[0, 1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold', pad=15)\naxes[0, 1].legend(fontsize=11)\naxes[0, 1].grid(True, alpha=0.3)\n\n# Accuracy progression\nepochs = range(1, len(train_accs) + 1)\naxes[1, 0].scatter(epochs, train_accs, label='Training', alpha=0.6, s=80, color='#e74c3c')\naxes[1, 0].scatter(epochs, val_accs, label='Validation', alpha=0.6, s=80, color='#3498db')\naxes[1, 0].plot(epochs, train_accs, alpha=0.3, color='#e74c3c')\naxes[1, 0].plot(epochs, val_accs, alpha=0.3, color='#3498db')\naxes[1, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\naxes[1, 0].set_title('Accuracy Progression', fontsize=14, fontweight='bold', pad=15)\naxes[1, 0].legend(fontsize=11)\naxes[1, 0].grid(True, alpha=0.3)\n\n# Overfitting gap\ngap = np.array(train_accs) - np.array(val_accs)\naxes[1, 1].plot(epochs, gap, linewidth=2.5, color='#9b59b6', alpha=0.8)\naxes[1, 1].axhline(y=0, color='green', linestyle='--', linewidth=2, alpha=0.6, label='No Gap')\naxes[1, 1].axhline(y=10, color='orange', linestyle='--', linewidth=2, alpha=0.6, label='10% Gap')\naxes[1, 1].fill_between(epochs, 0, gap, alpha=0.3, color='#9b59b6')\naxes[1, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Accuracy Gap (%)', fontsize=12, fontweight='bold')\naxes[1, 1].set_title('Overfitting Analysis', fontsize=14, fontweight='bold', pad=15)\naxes[1, 1].legend(fontsize=11)\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/training_curves.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(\"  âœ“ Training curves saved\")\n\n\n# PER-CLASS PERFORMANCE\nprint(\"\\nGenerating per-class performance...\")\n\ndf = pd.DataFrame({\n    'Class': class_names,\n    'Precision': test_metrics['precision_per_class'] * 100,\n    'Recall': test_metrics['recall_per_class'] * 100,\n    'F1-Score': test_metrics['f1_per_class'] * 100,\n    'Support': test_metrics['support_per_class']\n})\ndf = df.sort_values('F1-Score', ascending=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(20, 12))\n\ny_pos = np.arange(len(df))\n\naxes[0].barh(y_pos - 0.2, df['Precision'], 0.2, label='Precision', alpha=0.8, color='#3498db')\naxes[0].barh(y_pos, df['Recall'], 0.2, label='Recall', alpha=0.8, color='#e74c3c')\naxes[0].barh(y_pos + 0.2, df['F1-Score'], 0.2, label='F1-Score', alpha=0.8, color='#2ecc71')\naxes[0].set_yticks(y_pos)\naxes[0].set_yticklabels([name[:30] for name in df['Class']], fontsize=8)\naxes[0].set_xlabel('Score (%)', fontsize=12, fontweight='bold')\naxes[0].set_title('Per-Class Performance', fontsize=14, fontweight='bold', pad=15)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3, axis='x')\naxes[0].set_xlim(0, 100)\n\naxes[1].barh(y_pos, df['Support'], alpha=0.7, color='#95a5a6')\naxes[1].set_yticks(y_pos)\naxes[1].set_yticklabels([name[:30] for name in df['Class']], fontsize=8)\naxes[1].set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\naxes[1].set_title('Sample Distribution', fontsize=14, fontweight='bold', pad=15)\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/per_class_metrics.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(\"  âœ“ Per-class metrics saved\")\n\n\n# SAMPLE PREDICTIONS\nprint(\"\\nGenerating sample predictions...\")\n\nsample_indices = random.sample(range(len(test_dataset)), 12)\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 15))\naxes = axes.flatten()\n\nfor i, idx in enumerate(sample_indices):\n    image, true_label = test_dataset[idx]\n    \n    img_display = image.clone().permute(1, 2, 0).numpy()\n    img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n    img_display = np.clip(img_display, 0, 1)\n    \n    with torch.no_grad():\n        image_tensor = image.unsqueeze(0).to(DEVICE)\n        outputs = model(image_tensor)\n        probs = torch.softmax(outputs, dim=1)\n        confidence, predicted = torch.max(probs, 1)\n    \n    true_class = class_names[true_label]\n    pred_class = class_names[predicted.item()]\n    conf = confidence.item()\n    \n    axes[i].imshow(img_display)\n    \n    if true_class == pred_class:\n        color = 'green'\n        marker = 'âœ“'\n        border_color = '#2ecc71'\n    else:\n        color = 'red'\n        marker = 'âœ—'\n        border_color = '#e74c3c'\n    \n    true_display = true_class[:25] + '...' if len(true_class) > 25 else true_class\n    pred_display = pred_class[:25] + '...' if len(pred_class) > 25 else pred_class\n    \n    title = f\"{marker}\\nTrue: {true_display}\\nPred: {pred_display}\\nConf: {conf:.2%}\"\n    axes[i].set_title(title, color=color, fontsize=10, fontweight='bold', pad=10)\n    axes[i].axis('off')\n    \n    for spine in axes[i].spines.values():\n        spine.set_edgecolor(border_color)\n        spine.set_linewidth(4)\n        spine.set_visible(True)\n\nplt.suptitle(f'Sample Predictions (Test Acc: {test_metrics[\"accuracy\"]:.2f}%)', \n             fontsize=18, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('/kaggle/working/sample_predictions.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(\"  âœ“ Sample predictions saved\")\n\n\n# CLASSIFICATION REPORT\nprint(\"\\nGenerating classification report...\")\n\nreport = classification_report(test_labels, test_preds, target_names=class_names, \n                               digits=3, zero_division=0)\nwith open('/kaggle/working/classification_report.txt', 'w') as f:\n    f.write(report)\n\nreport_dict = classification_report(test_labels, test_preds, target_names=class_names, \n                                   output_dict=True, zero_division=0)\nreport_df = pd.DataFrame(report_dict).transpose()\nreport_df.to_csv('/kaggle/working/classification_report.csv')\n\nprint(\"  âœ“ Classification report saved\")\n\n# ============================================================\n# METRICS SUMMARY\n# ============================================================\nsummary_data = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n    'Test Set': [\n        f\"{test_metrics['accuracy']:.2f}%\",\n        f\"{test_metrics['precision']:.2f}%\",\n        f\"{test_metrics['recall']:.2f}%\",\n        f\"{test_metrics['f1']:.2f}%\"\n    ],\n    'Validation Set': [\n        f\"{val_metrics['accuracy']:.2f}%\",\n        f\"{val_metrics['precision']:.2f}%\",\n        f\"{val_metrics['recall']:.2f}%\",\n        f\"{val_metrics['f1']:.2f}%\"\n    ]\n}\n\nsummary_df = pd.DataFrame(summary_data)\nsummary_df.to_csv('/kaggle/working/metrics_summary.csv', index=False)\n\n\n# FINAL SUMMARY\nprint(\"EVALUATION COMPLETE!\")\n\nprint(\"\\nðŸ“ SAVED FILES:\")\nprint(\"  1. best_model.pth - Best model checkpoint\")\nprint(\"  2. confusion_matrix_test.png\")\nprint(\"  3. confusion_matrix_val.png\")\nprint(\"  4. training_curves.png\")\nprint(\"  5. per_class_metrics.png\")\nprint(\"  6. sample_predictions.png\")\nprint(\"  7. classification_report.txt\")\nprint(\"  8. classification_report.csv\")\nprint(\"  9. metrics_summary.csv\")\n\nprint(\"\\nâœ… REQUIREMENT CHECK:\")\nif test_metrics['accuracy'] >= 75.0:\n    print(f\"  âœ“ Test Accuracy: {test_metrics['accuracy']:.2f}% â‰¥ 75% - TARGET MET! ðŸŽ‰\")\nelse:\n    print(f\"  âœ— Test Accuracy: {test_metrics['accuracy']:.2f}% < 75%\")\n    print(f\"  Gap: {75.0 - test_metrics['accuracy']:.2f}%\")\n\nprint(\"\\nðŸ“Š FINAL METRICS SUMMARY:\")\nprint(f\"  Training Time:     {total_training_time/60:.1f} minutes\")\nprint(f\"  Epochs Trained:    {len(train_accs)}\")\nprint(f\"  Best Epoch:        {best_epoch}\")\nprint(f\"  Best Val Acc:      {best_val_acc:.2f}%\")\nprint(f\"  Test Accuracy:     {test_metrics['accuracy']:.2f}%\")\nprint(f\"  Test Precision:    {test_metrics['precision']:.2f}%\")\nprint(f\"  Test Recall:       {test_metrics['recall']:.2f}%\")\nprint(f\"  Test F1-Score:     {test_metrics['f1']:.2f}%\")\n\nprint(\"\\nðŸ† TOP 5 BEST PERFORMING CLASSES:\")\ntop5 = df.tail(5)[['Class', 'F1-Score']].to_string(index=False, header=False)\nprint(top5)\n\nprint(\"\\nâš ï¸ BOTTOM 5 WORST PERFORMING CLASSES:\")\nbottom5 = df.head(5)[['Class', 'F1-Score']].to_string(index=False, header=False)\nprint(bottom5)\n\nprint(\"ALL DONE!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T09:12:27.809212Z","iopub.execute_input":"2026-01-30T09:12:27.809886Z","iopub.status.idle":"2026-01-30T11:14:14.085052Z","shell.execute_reply.started":"2026-01-30T09:12:27.809857Z","shell.execute_reply":"2026-01-30T11:14:14.084117Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nConfiguration:\n  Image Size: 256x256\n  Batch Size: 64\n  Epochs: 25 (max, with early stopping)\n  Learning Rate: 0.0003\n  Early Stop Patience: 7\n\n============================================================\nLOADING DATASET\n============================================================\nTotal images: 43,444\nNumber of classes: 38\nTrain: 34,755 | Val: 4,344 | Test: 4,345\nTotal parameters: 25,887,014\n\n============================================================\nSTARTING TRAINING\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 01/25: Train Loss: 2.1209, Train Acc: 53.34% | Val Loss: 1.7698, Val Acc: 60.70% | LR: 0.000120 | Time: 4.9m\n  âœ“ New best! Val Acc: 60.70%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 02/25: Train Loss: 1.3605, Train Acc: 76.67% | Val Loss: 1.1911, Val Acc: 81.10% | LR: 0.000180 | Time: 4.8m\n  âœ“ New best! Val Acc: 81.10%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 03/25: Train Loss: 1.1638, Train Acc: 82.74% | Val Loss: 1.1392, Val Acc: 82.71% | LR: 0.000240 | Time: 4.8m\n  âœ“ New best! Val Acc: 82.71%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 04/25: Train Loss: 1.0911, Train Acc: 85.36% | Val Loss: 1.2997, Val Acc: 78.13% | LR: 0.000300 | Time: 4.8m\n  No improvement (1/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 05/25: Train Loss: 1.0583, Train Acc: 86.47% | Val Loss: 0.9922, Val Acc: 87.94% | LR: 0.000300 | Time: 4.8m\n  âœ“ New best! Val Acc: 87.94%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 06/25: Train Loss: 0.9889, Train Acc: 89.02% | Val Loss: 1.0095, Val Acc: 88.05% | LR: 0.000298 | Time: 4.8m\n  âœ“ New best! Val Acc: 88.05%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 07/25: Train Loss: 0.9492, Train Acc: 90.29% | Val Loss: 0.9617, Val Acc: 89.46% | LR: 0.000293 | Time: 4.8m\n  âœ“ New best! Val Acc: 89.46%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 08/25: Train Loss: 0.9070, Train Acc: 91.75% | Val Loss: 0.8645, Val Acc: 92.93% | LR: 0.000284 | Time: 4.8m\n  âœ“ New best! Val Acc: 92.93%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 09/25: Train Loss: 0.8783, Train Acc: 92.94% | Val Loss: 0.9752, Val Acc: 89.43% | LR: 0.000271 | Time: 4.8m\n  No improvement (1/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/25: Train Loss: 0.8509, Train Acc: 93.86% | Val Loss: 0.8964, Val Acc: 91.69% | LR: 0.000256 | Time: 4.8m\n  No improvement (2/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/25: Train Loss: 0.8248, Train Acc: 94.88% | Val Loss: 0.8284, Val Acc: 94.11% | LR: 0.000238 | Time: 4.8m\n  âœ“ New best! Val Acc: 94.11%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/25: Train Loss: 0.8049, Train Acc: 95.51% | Val Loss: 0.8072, Val Acc: 94.94% | LR: 0.000218 | Time: 4.8m\n  âœ“ New best! Val Acc: 94.94%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/25: Train Loss: 0.7857, Train Acc: 96.12% | Val Loss: 0.7973, Val Acc: 95.47% | LR: 0.000196 | Time: 4.8m\n  âœ“ New best! Val Acc: 95.47%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/25: Train Loss: 0.7695, Train Acc: 96.77% | Val Loss: 0.8010, Val Acc: 95.12% | LR: 0.000173 | Time: 4.8m\n  No improvement (1/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15/25: Train Loss: 0.7486, Train Acc: 97.49% | Val Loss: 0.7977, Val Acc: 95.49% | LR: 0.000150 | Time: 4.8m\n  âœ“ New best! Val Acc: 95.49%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16/25: Train Loss: 0.7355, Train Acc: 97.97% | Val Loss: 0.7784, Val Acc: 96.22% | LR: 0.000127 | Time: 4.8m\n  âœ“ New best! Val Acc: 96.22%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17/25: Train Loss: 0.7214, Train Acc: 98.49% | Val Loss: 0.8006, Val Acc: 95.40% | LR: 0.000104 | Time: 4.8m\n  No improvement (1/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18/25: Train Loss: 0.7111, Train Acc: 98.83% | Val Loss: 0.7433, Val Acc: 97.63% | LR: 0.000082 | Time: 4.8m\n  âœ“ New best! Val Acc: 97.63%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19/25: Train Loss: 0.7003, Train Acc: 99.22% | Val Loss: 0.7731, Val Acc: 96.55% | LR: 0.000062 | Time: 4.8m\n  No improvement (1/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20/25: Train Loss: 0.6919, Train Acc: 99.53% | Val Loss: 0.7661, Val Acc: 96.62% | LR: 0.000044 | Time: 4.8m\n  No improvement (2/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 21/25: Train Loss: 0.6872, Train Acc: 99.69% | Val Loss: 0.7350, Val Acc: 97.63% | LR: 0.000029 | Time: 4.8m\n  No improvement (3/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 22/25: Train Loss: 0.6833, Train Acc: 99.86% | Val Loss: 0.7209, Val Acc: 98.32% | LR: 0.000016 | Time: 4.8m\n  âœ“ New best! Val Acc: 98.32%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23/25: Train Loss: 0.6820, Train Acc: 99.89% | Val Loss: 0.7330, Val Acc: 97.79% | LR: 0.000007 | Time: 4.8m\n  No improvement (1/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 24/25: Train Loss: 0.6806, Train Acc: 99.94% | Val Loss: 0.7335, Val Acc: 97.74% | LR: 0.000002 | Time: 4.8m\n  No improvement (2/7)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 25/25: Train Loss: 0.6802, Train Acc: 99.95% | Val Loss: 0.7305, Val Acc: 97.91% | LR: 0.000001 | Time: 4.8m\n  No improvement (3/7)\nTraining completed in 120.9 minutes\nBest validation accuracy: 98.32% at epoch 22\n\nCollecting predictions...\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nCalculating metrics...\n\nðŸ“Š TEST SET METRICS:\n  Accuracy:  97.95%\n  Precision: 97.98%\n  Recall:    97.95%\n  F1-Score:  97.95%\n\nðŸ“Š VALIDATION SET METRICS:\n  Accuracy:  98.32%\n  Precision: 98.35%\n  Recall:    98.32%\n  F1-Score:  98.32%\n\nGenerating confusion matrices...\n  âœ“ Confusion matrices saved\n\nGenerating training curves...\n  âœ“ Training curves saved\n\nGenerating per-class performance...\n  âœ“ Per-class metrics saved\n\nGenerating sample predictions...\n  âœ“ Sample predictions saved\n\nGenerating classification report...\n  âœ“ Classification report saved\nEVALUATION COMPLETE!\n\nðŸ“ SAVED FILES:\n  1. best_model.pth - Best model checkpoint\n  2. confusion_matrix_test.png\n  3. confusion_matrix_val.png\n  4. training_curves.png\n  5. per_class_metrics.png\n  6. sample_predictions.png\n  7. classification_report.txt\n  8. classification_report.csv\n  9. metrics_summary.csv\n\nâœ… REQUIREMENT CHECK:\n  âœ“ Test Accuracy: 97.95% â‰¥ 75% - TARGET MET! ðŸŽ‰\n\nðŸ“Š FINAL METRICS SUMMARY:\n  Training Time:     120.9 minutes\n  Epochs Trained:    25\n  Best Epoch:        22\n  Best Val Acc:      98.32%\n  Test Accuracy:     97.95%\n  Test Precision:    97.98%\n  Test Recall:       97.95%\n  Test F1-Score:     97.95%\n\nðŸ† TOP 5 BEST PERFORMING CLASSES:\n               Corn_(maize)___Common_rust_ 100.0\n                    Corn_(maize)___healthy 100.0\nGrape___Leaf_blight_(Isariopsis_Leaf_Spot) 100.0\n         Cherry_(including_sour)___healthy 100.0\n                          Tomato___healthy 100.0\n\nâš ï¸ BOTTOM 5 WORST PERFORMING CLASSES:\n                          Apple___Cedar_apple_rust 87.804878\nCorn_(maize)___Cercospora_leaf_spot Gray_leaf_spot 88.888889\n                                  Potato___healthy 88.888889\n               Corn_(maize)___Northern_Leaf_Blight 93.333333\n                             Tomato___Early_blight 94.520548\nALL DONE!\n","output_type":"stream"}],"execution_count":3}]}